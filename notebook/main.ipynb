{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51baa77a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from typing import TypedDict, Annotated\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "from langchain_openai import ChatOpenAI,OpenAIEmbeddings\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "from langchain_community.vectorstores.pgvector import PGVector\n",
    "\n",
    "# from langchain_postgres import PGVector\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph,add_messages,START,END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain.messages import RemoveMessage # to delete something from state permenantly\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "from typing import TypedDict, Annotated\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=r\"C:\\Users\\hasee\\Desktop\\Legal Chatbot\\.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdde66b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "CONNECTION_STRING = os.environ.get(\"CONNECTION_STRING\",\"\")\n",
    "SUPERBASE_SERVICE_ROLE_KEY = os.environ.get(\"SUPABASE_SERVICE_ROLE_KEY\",\"\")\n",
    "SUPABASE_URL = os.environ.get(\"SUPABASE_URL\",\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e41acd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from supabase import create_client\n",
    "\n",
    "supabase_client = create_client(SUPABASE_URL,SUPERBASE_SERVICE_ROLE_KEY)\n",
    "print(\"Succefully coonectd to Supabase client\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a848c4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# chatting llm\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\",temperature=0,streaming=True)\n",
    "#embedding llm\n",
    "EMBEDDING = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a97f548",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    documents_path:str\n",
    "    documents:list[Document]\n",
    "    chunks:list[Document] \n",
    "    collection_name:str\n",
    "    retrieved_docs:list[Document]\n",
    "    context: str \n",
    "    answer:str\n",
    "    messages: Annotated[list[BaseMessage], add_messages]\n",
    "    doc_id:str\n",
    "    summary:str\n",
    "    vectorstore_uploaded:bool\n",
    "    rewritten_query:str\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "        You are an expert Legal AI Assistant for Pakistan. Your task is to answer legal questions based strictly on the provided context.\n",
    "\n",
    "        Instructions:\n",
    "        1. Source-Based Answering: Answer the question using ONLY the information provided in the Context below. Do not use outside knowledge.\n",
    "        2. Specific Legal Citations: When making a statement, you must cite the specific legal authority found in the text (e.g., \"Article 6 of the Constitution\", \"Section 302 of PPC\", \"Clause 3\"). \n",
    "        3. Citation Format: Format citations as: [Legal Reference]** (Found in: Chunk ID/Source).\n",
    "            Example: \"Every citizen has the right to a fair trial as per Article 10-A (Source: Chunk 2, constitution.pdf)..\"\n",
    "        4. No Hallucinations:** If the provided context does not contain the answer, state: \"The provided context does not contain sufficient information to answer this question.\"\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question:\n",
    "        {question}\n",
    "\n",
    "        Answer:\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe83750b",
   "metadata": {},
   "source": [
    "# Document ID generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def get_file_hash(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    It reads a file (like a PDF) and generates a SHA-256 hash,\n",
    "      which is a fixed-length unique string representing the file’s content.\n",
    "\n",
    "    If:\n",
    "    The file content is exactly the same → hash is the same\n",
    "    Even 1 byte changes → hash is completely different\n",
    "    \"\"\"\n",
    "    hasher = hashlib.sha256()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        hasher.update(f.read())\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "# get_file_hash() reads the entire file content and produces a SHA-256 hash.\n",
    "# SHA-256 guarantees:\n",
    "# Same content → same hash → same doc_id\n",
    "# Even 1 byte difference → completely different hash\n",
    "# So if a user uploads the same PDF file again, the hash will be identical → same doc_id.\n",
    "\n",
    "\n",
    "# Even one single byte change in the file will produce a completely different hash.\n",
    "# That means if you change one word in the PDF, the doc_id will be different, because the file content is no longer exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1448f4e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "        \n",
    "def set_doc_id(state:AgentState):\n",
    "    path = os.path.abspath(state[\"documents_path\"])\n",
    "    \n",
    "    if not os.path.isfile(path):\n",
    "        raise ValueError(\"Directoy uploaded not supported with hashing yet\")\n",
    "    state[\"doc_id\"] = get_file_hash(path)\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7beaf85",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def check_pdf_already_uploaded(state:AgentState):\n",
    "    \"\"\"Checkif PDF already exist in SUpbase\"\"\"\n",
    "    # first check if vectostore already exist\n",
    "    if state.get(\"vectorstore_uploaded\"):\n",
    "        return state\n",
    "    # it aslo check if vectorstore exist in database\n",
    "    response = (self.supabase_client.table(\"documents\").select(\"doc_id\").eq(\"doc_id\",state[\"doc_id\"]).limit(1).execute())\n",
    "    if response.data:\n",
    "        print(\"Pdf already exist in supbase skipping documnet ingesion...\")\n",
    "        state[\"vectorstore_uploaded\"] = True\n",
    "    else:\n",
    "        state[\"vectorstore_uploaded\"] = False\n",
    "    return state  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32947e5a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def document_ingestion(state: AgentState):\n",
    "    if state.get(\"vectorstore_uploaded\"):\n",
    "        print(\"Skipping vectoingestion - PDF already exist\")\n",
    "        state[\"vectorstore_uploaded\"] = True\n",
    "        return state\n",
    "    \n",
    "    path = os.path.abspath(state[\"documents_path\"])  # ensure absolute\n",
    "\n",
    "    if not os.path.isfile(path):\n",
    "        raise ValueError(f\"Invalid documents_path: {path}\")\n",
    "    \n",
    "\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "    chunks = splitter.split_documents(documents)\n",
    "\n",
    "    # langchain chunk metadata is first updated\n",
    "    # langchain chunk metadata (each chunk of document will have this metadata (it will not have page content - only metadata))\n",
    "    for i,chunk in enumerate(chunks):\n",
    "        source_path = chunk.metadata.get(\"source\",\"\")\n",
    "        file_name = os.path.basename(source_path) if source_path else \"unknow.pdf\"\n",
    "\n",
    "        metadata = {\n",
    "            \"doc_id\":state[\"doc_id\"],\n",
    "            \"chunk_index\":i,\n",
    "            \"file_name\":file_name,\n",
    "            \"page\":chunk.metadata.get(\"page\")  \n",
    "        }\n",
    "        # update langchian chunk metadata usd by pgvector\n",
    "        chunk.metadata.update(metadata)\n",
    "\n",
    "    vectorstore = PGVector(\n",
    "        connection=CONNECTION_STRING,\n",
    "        collection_name=state[\"collection_name\"],\n",
    "        embeddings=self.embedding_model,\n",
    "        use_jsonb=True,\n",
    "        engine_args={\"poolclass\": NullPool}  # disable pooling\n",
    "    )\n",
    "    batch_size=50\n",
    "    # upload embedding \n",
    "    for i in tqdm(range(0, len(chunks), batch_size), desc=\"Uploading chunks\"):\n",
    "        batch = chunks[i:i + batch_size]\n",
    "        vectorstore.add_documents(batch)\n",
    "    \n",
    "\n",
    "    # Insert metadata to supbase table\n",
    "    rows = [{\n",
    "            \"doc_id\": state[\"doc_id\"],\n",
    "            \"chunk_index\": i,\n",
    "            \"file_name\": chunk.metadata[\"file_name\"],\n",
    "            \"page\": chunk.metadata.get(\"page\"),\n",
    "            \"content\": chunk.page_content,\n",
    "    } for i,chunk in enumerate(chunks)\n",
    "    ]\n",
    "    if rows:\n",
    "        self.supabase_client.table(\"documents\").insert(rows).execute()\n",
    "\n",
    "    print(f\"Uploaded {len(chunks)} chunks\")\n",
    "\n",
    "    state[\"vectorstore_uploaded\"] = True\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cf8e67",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def query_rewriter(state: AgentState):\n",
    "    \"\"\"Rewrite follow-up questions to be standalone using conversation context\"\"\"\n",
    "    \n",
    "    human_messages = [m for m in state.get(\"messages\", []) if isinstance(m, HumanMessage)]\n",
    "    current_query = human_messages[-1].content\n",
    "    \n",
    "    # If there's conversation history, rewrite the query\n",
    "    if len(state.get(\"messages\", [])) > 1:\n",
    "        \n",
    "        # Build conversation context\n",
    "        memory_text = state.get(\"summary\") or \"\"\n",
    "        if not memory_text:\n",
    "            conversation_history = []\n",
    "            for m in state.get(\"messages\", [])[:-1]:  # Exclude current question\n",
    "                role = \"User\" if isinstance(m, HumanMessage) else \"Assistant\"\n",
    "                conversation_history.append(f\"{role}: {m.content}\")\n",
    "            memory_text = \"\\n\".join(conversation_history)\n",
    "        \n",
    "        # Rewrite query to be standalone\n",
    "        rewrite_prompt = f\"\"\"Given this conversation history:\n",
    "            {memory_text}\n",
    "\n",
    "            Rewrite the following question to be standalone (include necessary context from history):\n",
    "            Question: {current_query}\n",
    "\n",
    "            Standalone question:\"\"\"\n",
    "                    \n",
    "        response = self.llm.invoke([HumanMessage(content=rewrite_prompt)])\n",
    "        rewritten_query = response.content.strip()\n",
    "        \n",
    "        print(f\"Original: {current_query}\")\n",
    "        print(f\"Rewritten: {rewritten_query}\")\n",
    "        \n",
    "        # Store rewritten query for retrieval\n",
    "        state[\"rewritten_query\"] = rewritten_query\n",
    "    else:\n",
    "        state[\"rewritten_query\"] = current_query\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d7c3ad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# We can add Metadata filtering Here\n",
    "# We can add Metadata filtering Here\n",
    "def retriever(self,state: AgentState):     \n",
    "    vectorstore = PGVector(\n",
    "        connection=CONNECTION_STRING,\n",
    "        collection_name=state[\"collection_name\"],\n",
    "        embeddings=self.embedding_model,\n",
    "        use_jsonb=True,\n",
    "        engine_args={\"poolclass\": NullPool}  # disable pooling\n",
    "    )\n",
    "    # Metadata filter for this specific PDF\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\n",
    "            \"k\": 5,\n",
    "            \"filter\": {\"doc_id\": state[\"doc_id\"]} # only search this pdf \n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Use rewritten query instead of original\n",
    "    query = state.get(\"rewritten_query\", state[\"messages\"][-1].content)\n",
    "    \n",
    "    retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "    state[\"retrieved_docs\"] = retrieved_docs\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0d0777",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# file_name is added during text splitting\n",
    "# page exists → PyPDFLoader adds this automatically\n",
    "def context_builder(state:AgentState):\n",
    "        retrieved_docs = state.get(\"retrieved_docs\",[])\n",
    "        if not state[\"retrieved_docs\"]:\n",
    "            state[\"context\"] = \"\"\n",
    "            state[\"answer\"] = (\"I could not find relevant information in the provided document.\")\n",
    "        else:\n",
    "\n",
    "            context = \"\\n\\n\".join(\n",
    "                f\"[Source: {doc.metadata.get('file_name', 'Unknown')} \"\n",
    "                f\"- Page {doc.metadata.get('page', 'N/A')}]\\n\"    # page no\n",
    "                f\"{doc.page_content}\"\n",
    "                for doc in retrieved_docs\n",
    "            )\n",
    "            state[\"context\"] = context\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7319e4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def summary_creation(state:AgentState):\n",
    "    existing_summary = state[\"summary\"] # we first load existing summary\n",
    "\n",
    "    # We have two scenrio:\n",
    "    # 1. We might already have summary\n",
    "    # 2. or We are Genrating summary fir the first time\n",
    "    if existing_summary:\n",
    "        prompt = (\n",
    "            f\"Existing summary:\\n{existing_summary}\\n\\n\"\n",
    "            \"Extend the summary using new conversation above\"\n",
    "        )\n",
    "    else:\n",
    "        prompt = \"summarize the conversation above\"\n",
    "\n",
    "    message_for_summary = state[\"messages\"] + [HumanMessage(content=prompt)]\n",
    "\n",
    "    print(\"Callin summary LLM\") # debugging\n",
    "    # generate summary\n",
    "    response = self.llm.invoke(message_for_summary)\n",
    "\n",
    "    # now delete the orignal messages that have been summarized\n",
    "    message_to_delete = state[\"messages\"][:-2] if len(state[\"messages\"]) > 2 else []\n",
    "\n",
    "    return {\n",
    "        \"summary\":response.content,\n",
    "        \"messages\":[RemoveMessage(id=m.id) for m in message_to_delete]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb7252f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def should_summzarizer(state:AgentState):\n",
    "    return len(state[\"messages\"]) > 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d4d21e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# cat node with memory\n",
    "def agent_response(state: AgentState):\n",
    "    \"\"\"\n",
    "    Generates the LLM response for the current query, injecting memory (summary or previous messages)\n",
    "    and RAG context into the prompt.\n",
    "    \"\"\"\n",
    "    context = state.get(\"context\", \"\")\n",
    "\n",
    "    # Get all human messages\n",
    "    human_messages = [m for m in state.get(\"messages\", []) if isinstance(m, HumanMessage)]\n",
    "    if not human_messages:\n",
    "        raise ValueError(\"No human message found in state for retrieval\")\n",
    "\n",
    "    query = human_messages[-1].content\n",
    "\n",
    "    prompt_messages = []\n",
    "\n",
    "    # Memory injection \n",
    "    # Use summary if it exists, otherwise include all previous messages\n",
    "    memory_text = state.get(\"summary\", \"\")\n",
    "    if not memory_text:\n",
    "        conversation_history = []\n",
    "        for m in state.get(\"messages\", []):\n",
    "            role = \"User\" if isinstance(m, HumanMessage) else \"Assistant\"\n",
    "            conversation_history.append(f\"{role}: {m.content}\")\n",
    "        memory_text = \"\\n\".join(conversation_history) if conversation_history else \"No previous conversation.\"\n",
    "\n",
    "    # Inject memory as system message\n",
    "    prompt_messages.append(SystemMessage(content=f\"Conversation Memory:\\n{memory_text}\"))\n",
    "\n",
    "    # RAG context + current query \n",
    "    formatted_prompt = PROMPT_TEMPLATE.format(\n",
    "        context=context,\n",
    "        question=query\n",
    "    )\n",
    "    prompt_messages.append(HumanMessage(content=formatted_prompt))\n",
    "\n",
    "    print(\"Calling Agent Response LLM\")  # debugging\n",
    "    response = self.llm.invoke(prompt_messages)\n",
    "\n",
    "    # Save AI response in state\n",
    "    state[\"messages\"].append(AIMessage(content=response.content))\n",
    "    state[\"answer\"] = response.content\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614e8211",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def conditional(state: AgentState):\n",
    "    if state.get(\"vectorstore_uploaded\", False):\n",
    "        return \"query_rewriter\"   # already exists → query\n",
    "    else:\n",
    "        return \"document_ingestion\"  # new → ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb33143",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# from langgraph.checkpoint.postgres import PostgresSaver\n",
    "# import psycopg\n",
    "\n",
    "# conn = psycopg.connect(\n",
    "#     CONNECTION_STRING,\n",
    "#     autocommit=True\n",
    "# )\n",
    "# # PostgresSaver --> LangGraph will automatically create its own tables in your Supabase Postgres database the first time it runs.\n",
    "# checkpointer = PostgresSaver(conn) \n",
    "# # It creates the internal tables in Supabase:\n",
    "# # checkpoints\n",
    "# # checkpoint_writes\n",
    "# # These tables store:\n",
    "# # AgentState snapshots\n",
    "# # Messages\n",
    "# # Node execution progress\n",
    "# # Thread state\n",
    "# checkpointer.setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbdb98a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "    # nodes\n",
    "workflow.add_node(\"document_ingestion\",document_ingestion)\n",
    "workflow.add_node(\"query_rewriter\", query_rewriter)\n",
    "workflow.add_node(\"retriever\", retriever)\n",
    "workflow.add_node(\"context_builder\", context_builder)\n",
    "workflow.add_node(\"agent_response\", agent_response)\n",
    "workflow.add_node(\"summarize\",summary_creation)\n",
    "workflow.add_node(\"check_pdf\",check_pdf_already_uploaded)\n",
    "workflow.add_node(\"set_doc_id\",set_doc_id)\n",
    "\n",
    "# edges\n",
    "workflow.add_edge(START, \"set_doc_id\")\n",
    "workflow.add_edge(\"set_doc_id\", \"check_pdf\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_pdf\",\n",
    "    nodes.conditional,\n",
    "    {\n",
    "        \"document_ingestion\": \"document_ingestion\",\n",
    "        \"query_rewriter\": \"query_rewriter\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# if new vector store path\n",
    "workflow.add_edge(\"document_ingestion\",\"query_rewriter\")\n",
    "\n",
    "workflow.add_edge(\"query_rewriter\", \"retriever\")\n",
    "workflow.add_edge(\"retriever\", \"context_builder\")\n",
    "workflow.add_edge(\"context_builder\", \"agent_response\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent_response\",\n",
    "    nodes.should_summzarizer,\n",
    "    {\n",
    "        True: \"summarize\",\n",
    "        False: END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"summarize\", END)\n",
    "\n",
    "app = workflow.compile(checkpointer=self.checkpointer)\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d67087",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Project root\n",
    "PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "\n",
    "file_path = os.path.join(PROJECT_ROOT,\"data\",\"Constitution and law\",\"PAKISTAN PENAL CODE.pdf\")\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(\"File is not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3670769d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Generate unique doc_id for PDF\n",
    "doc_id = get_file_hash(file_path)\n",
    "\n",
    "# Create a collection name based on file name\n",
    "collection_name = (os.path.splitext(os.path.basename(file_path))[0].lower().replace(\" \", \"_\"))\n",
    "\n",
    "# Thread ID for chat persistence\n",
    "thread_id = \"user-123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7775ec4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Connect to Supabase Postgres for checkpointing\n",
    "with PostgresSaver.from_conn_string(CONNECTION_STRING) as checkpointer:\n",
    "    checkpointer.setup()  # run once\n",
    "\n",
    "    # Build the workflow graph\n",
    "    graph = GraphBuilder(checkpointer=checkpointer)()  # use as function\n",
    "\n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "    # ===== FIRST MESSAGE =====\n",
    "    result = graph.invoke(\n",
    "        {\n",
    "            \"documents_path\": file_path,\n",
    "            \"doc_id\": doc_id,\n",
    "            \"collection_name\": collection_name,\n",
    "            \"messages\": [HumanMessage(content=\"What is punishment for making false claim in court?\")],\n",
    "            \"summary\": \"\"},\n",
    "        config=config\n",
    "    ) \n",
    "    print(\"Answer 1:\", result[\"answer\"])\n",
    "\n",
    "\n",
    "    # ===== SECOND MESSAGE =====\n",
    "\n",
    "    initial_state = {\n",
    "    \"doc_id\": doc_id,\n",
    "    \"collection_name\": collection_name,  # Use existing vectorstore\n",
    "    \"messages\": [HumanMessage(content=\"What is the penalty for that?\")],  # Follow-up question\n",
    "}\n",
    "    result = graph.invoke(initial_state,config=config)\n",
    "    print(\"Answer 2:\", result[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab40aa8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
